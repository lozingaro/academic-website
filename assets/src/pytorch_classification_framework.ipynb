{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_classification_framework.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-83qDrqWNNE","executionInfo":{"status":"ok","timestamp":1625131253417,"user_tz":-120,"elapsed":4128,"user":{"displayName":"Stefano Pio Zingaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLM3x4R4qM7jmWr46nGGXWCTy747J476ywOHVDug=s64","userId":"16618396522953995505"}},"outputId":"3d713d26-8b60-45e3-a31a-a05f1699e5a2"},"source":["!pip install timm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting timm\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/fc/606bc5cf46acac3aa9bd179b3954433c026aaf88ea98d6b19f5d14c336da/timm-0.4.12-py3-none-any.whl (376kB)\n","\u001b[K     |████████████████████████████████| 378kB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: timm\n","Successfully installed timm-0.4.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sA6f5ILebHXg","executionInfo":{"status":"ok","timestamp":1625134013339,"user_tz":-120,"elapsed":566,"user":{"displayName":"Stefano Pio Zingaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLM3x4R4qM7jmWr46nGGXWCTy747J476ywOHVDug=s64","userId":"16618396522953995505"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from collections import OrderedDict\n","\n","import timm"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gO3qE6x21VH6"},"source":["To build a model based on $n$ identical blocks, we use a wrapper for a generic block instance (a `torch.nn.Module`) and add a normalization layer at the bottom of the pipeline.  "]},{"cell_type":"code","metadata":{"id":"FR6N5ZqG-IXK"},"source":["class SimpleBlockModel(nn.Module):\n","  \"\"\"\n","  \"\"\"\n","  def __init__(\n","      self,\n","      block_layer:nn.Module,\n","      features_dim:int,\n","      batch_norm:nn.Module=None,\n","      p_drop:float=.1,\n","  ):\n","    super().__init__()\n","    self.batch_norm = batch_norm or nn.BatchNorm1d\n","    self.dropout = nn.Dropout(p_drop)\n","    self.act_fn = act_fn or nn.Relu\n","\n","    self.batch_norm = batch_norm(features_dim)\n","    self.block_layer = block_layer\n","    self.proj = nn.utils.weight_norm(nn.Linear(features_dim, features_dim)) # weight norm from https://www.kaggle.com/shahules/pytorch-entity-embedding#Model\n","\n","def forward(self, x):\n","  x = block_layer(x)\n","  x = self.batch_norm(x)\n","  x = self.dropout(x)\n","  return act_fn(self.proj(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BS86mpKjFZhB"},"source":["class MultiBlockModel(nn.Module):\n","    \"\"\"\n","    MultiBlockModel is a stack of n blocks with a final normalization layer.\n","    \"\"\"\n","    def __init__(\n","        self, \n","        block:nn.Module, \n","        num_blocks:int=1\n","    ):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([copy.deepcopy(block) for _ in range(num_blocks)])\n","        self.norm = LayerNorm(block.size)\n","        \n","    def forward(self, x):\n","      for block in self.blocks:\n","        x = block(x)\n","      return self.norm(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D--IiIY2bLi6"},"source":["class EncoderDecoderModel(nn.Module):\n","  \"\"\"\n","  An encoder/decoder architecture for classification (-ish).\n","  \"\"\"\n","  def __init__(\n","      self, \n","      encoder:nn.Module,\n","      decoder:nn.Module,\n","      classifier:nn.Module\n","  ):\n","    super().__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.classifier = classifier\n","\n","  def forward(self, \n","              x, \n","              return_representation=False, \n","              return_features=False, \n","              return_prob=True):\n","    representation = self.encoder(x)\n","    if return_representation:\n","      return representation\n","    features = self.decoder(repr)\n","    if return_features:\n","      return features\n","    y = self.classifier(features)\n","    if return_prob:\n","      return F.log_softmax(y) # obtain model confidence (for multiclass problems)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTGd1Nemg22p"},"source":["class CatModel(nn.Module):\n","  def __init__(\n","      self,\n","      left_model:nn.Module,\n","      right_model:nn.Module,\n","  ):\n","    super().__init__()\n","    self.left_model = left_model\n","    self.right_model = right_model\n","\n","  def forward(self, left_x, right_x):\n","    left_x = left_model(left_x)\n","    right_x = right_model(right_x)\n","    return torch.cat((left_x, right_x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q91-aKxUfNjC"},"source":["import math\n","\n","class EmdebbingModel(nn.Module):\n","  def __init__(\n","      self, \n","      num_embeddings:int, \n","      embedding_dim:int\n","  ):\n","    \"\"\"\n","    num_embeddings: size of the dictionary of embeddings\n","    embedding_dim: the size of each embedding vector \n","    \"\"\"\n","    super().__init__()\n","    self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n","    self.embedding_dim = embedding_dim\n","\n","  def forward(self, x):\n","    return self.embedding(x) * math.sqrt(self.embedding_dim) # sqrt from \"attention is all you need\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbXjZYqMB-uS"},"source":["class ResidualBlockModel(nn.Module):\n","  def __init__(\n","      self,\n","      block_layer:nn.Module,\n","      norm_layer:nn.Module,\n","      in_channels:int,\n","      out_channels:int,\n","  ):\n","    super().__init__()\n","    self.block_layer1 = block_layer(in_channels, out_channels)\n","    self.batch_norm = norm_layer(out_channels)\n","    self.act_fn = act_fn or nn.ReLU(inplace=True)\n","    self.block_layer2 = block_layer(out_channels, out_channels)\n","    \n","  def forward(self, x):\n","    identity = x\n","\n","    y = self.block_layer1(x)\n","    y = self.batch_norm(y)\n","    y = self.act_fn(y)\n","\n","    y = self.block_layer2(y)\n","    y = self.batch_norm(y)\n","\n","    y += identity\n","    return self.act_fn(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxDtYNEsW8zf"},"source":["class ResidualSimpleBlockModel(nn.Module):\n","  def __init__(\n","      self,\n","      block_layer:nn.Module,\n","      in_channels:int,\n","      out_channels:int,\n","      norm_layer:nn.Module=None,\n","      act_fn:nn.Module=None,\n","  ):\n","    super().__init__()\n","    self.block_layer = block_layer(in_channels, out_channels)\n","    self.batch_norm = norm_layer or nn.BatchNorm1d\n","    self.batch_norm = batch_norm(out_channels)\n","    self.act_fn = act_fn or nn.ReLU(inplace=True)\n","    self.proj = nn.utils.weight_norm(nn.Linear(out_channels, out_channels)) # weight norm from https://www.kaggle.com/shahules/pytorch-entity-embedding#Model\n","    \n","  def forward(self, x):\n","    identity = x\n","\n","    y = self.block_layer(x)\n","    y = self.batch_norm(y)\n","    y = self.act_fn(self.proj(y))\n","\n","    y += identity\n","    return self.act_fn(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dDi2VqGiGa1"},"source":["class IdentityModel(nn.Module):\n","  def __init__(\n","      self,\n","  ):\n","    super().__init__()\n","    self.identity = nn.Identity()\n","    \n","  def forward(self, x):\n","    return self.identity(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vdEvWJJ_9dDR"},"source":["#  Test"]},{"cell_type":"code","metadata":{"id":"wPSGvkPjhb8L","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1625134134897,"user_tz":-120,"elapsed":495,"user":{"displayName":"Stefano Pio Zingaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLM3x4R4qM7jmWr46nGGXWCTy747J476ywOHVDug=s64","userId":"16618396522953995505"}},"outputId":"b19f378e-b172-4cfa-f9ac-a17467b6112c"},"source":["from timm.models.layers.classifier import ClassifierHead\n","\n","model = EncoderDecoderModel(\n","    encoder=CatModel(\n","        left_model=EmdebbingModel(\n","            num_embeddings=input_dim,\n","            embedding_dim=features_dim,\n","        ),\n","        right_model=IdentityModel()\n","    ),\n","    decoder=MultiBlockModel(\n","        block=ResidualBlockModel(\n","            block_layer=nn.Linear,\n","            norm_layer=nn.BatchNorm1d,\n","            in_channels=features_dim,\n","            out_channels=features_dim,\n","        ),\n","        num_blocks=1\n","    ),\n","    classifier=ClassifierHead(\n","        in_chs=features_dim, \n","        num_classes=num_classes, \n","        pool_type='avg', \n","        drop_rate=0., \n","        use_conv=False\n","    )\n",")"],"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1e4ed9d07d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = EncoderDecoderModel(\n\u001b[0m\u001b[1;32m      4\u001b[0m     encoder=CatModel(\n\u001b[1;32m      5\u001b[0m         left_model=EmdebbingModel(\n","\u001b[0;31mNameError\u001b[0m: name 'EncoderDecoderModel' is not defined"]}]}]}